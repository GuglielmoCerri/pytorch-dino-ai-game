{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8854aa7b-5f0d-4d93-848f-5eeda754aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from torchvision.transforms import CenterCrop, Resize, Compose, ToTensor, Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba21db-dbdb-4e6c-8740-52b2ed704ad1",
   "metadata": {},
   "source": [
    "#### Now, let’s create an image transformation pipeline that’s required for EfficientNet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace79cec-a5c7-4ac1-b309-5045af7557a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Compose([\n",
    "    Resize((480,480)),\n",
    "    CenterCrop(480),\n",
    "    Normalize(mean =[0.485, 0.456, 0.406], std =[0.229, 0.224, 0.225] )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee68e6-b728-4024-af9b-a0578efad71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "\n",
    "        self.key_frame = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.to_list()\n",
    "    \n",
    "        img_name = os.path.join(self.root_dir, self.key_frame.iloc[idx,0])\n",
    "        image = Image.open(img_name)\n",
    "        image = ToTensor()(image)\n",
    "        label = torch.tensor(self.key_frame.iloc[idx, 1])\n",
    "\n",
    "        if self.transform: \n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d7405-ffef-4587-9cfd-6ff2a9c81d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_frame = pd.read_csv(\"labels.csv\") #importing the csv file with the labels of the key frames\n",
    "train,test = train_test_split(key_frame, test_size = 0.2)  #splitting the data into train and test sets\n",
    "train = pd.DataFrame(train) \n",
    "test = pd.DataFrame(test)\n",
    "\n",
    "batch_size = 4\n",
    "trainset = DinoDataset(root_dir = \"captures\", dataframe = train, transform = transformer)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size) \n",
    "\n",
    "testset = DinoDataset(root_dir = \"captures\", dataframe = test, transform = transformer)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f1f54-a450-4751-9108-3c6b1c041282",
   "metadata": {},
   "source": [
    "#### Let’s check out the images in one of the batches in the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498272d-b389-4e4e-a772-a0ec64201dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "for i in range(len(images)):\n",
    "        ax = plt.subplot(2, 4, i + 1)\n",
    "        image = (images[i].permute(1,2,0)*255.0).cpu()\n",
    "        ax.set_title(labels[i].item(), fontsize=20)  # Setting the title of the subplot\n",
    "        ax.set_xticklabels([])   # Removing the x-axis labels\n",
    "        ax.set_yticklabels([])   # Removing the y-axis labels\n",
    "        plt.imshow(image)        # Plotting the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ff2d0-62c4-4065-bdcf-291ba90c127e",
   "metadata": {},
   "source": [
    "#### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d01ae5-e0e7-40f1-8e4c-fa3f67faeedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = torchvision.models.efficientnet_v2_s()\n",
    "model.classifier = torch.nn.Linear(in_features = 1280, out_features = 2)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67167967-8d72-4654-8109-c873d37cc318",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce68df-952b-4f99-b6ec-df212bd74a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15  # number of training passes over the mini batches\n",
    "loss_container = [] # container to store the loss values after each epoch\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(trainloader, position=0, leave=True):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    loss_container.append(running_loss)\n",
    "\n",
    "    print(f'[{epoch + 1}] | loss: {running_loss / len(trainloader):.3f}')\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# plot the loss curve\n",
    "plt.plot(np.linspace(1, epochs, epochs).astype(int), loss_container)\n",
    "\n",
    "# clean up the gpu memory \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b4bb9-5add-4d68-a6a3-b2c8cf5c62b0",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a44754-a59f-45f2-b5fe-a60433687f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'efficientnet_s.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f119e-29d3-479e-8359-a880def5cc66",
   "metadata": {},
   "source": [
    "#### Testing the Model Performance\n",
    "Let’s load a new EfficientNet Model that uses the weights we saved in the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf65fb-5574-4cd2-b4ba-a9d973125cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = torchvision.models.efficientnet_v2_s()\n",
    "saved_model.classifier = torch.nn.Linear(in_features = 1280, out_features = 2)\n",
    "saved_model.load_state_dict(torch.load(PATH))\n",
    "saved_model = saved_model.to(device)\n",
    "saved_mode = saved_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb8c0d9-7b3e-4310-b5ff-eca0137e1794",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "  for data in tqdm(testloader):\n",
    "    images,labels = data\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = saved_model(images)\n",
    "    predicted = torch.softmax(outputs,dim = 1).argmax(dim = 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'\\\\n Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33d089-f23b-447f-99c6-ab2dc80fd97b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "dino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
